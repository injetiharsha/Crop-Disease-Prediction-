{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/injetiharsha/Crop-Disease-Prediction-/blob/main/Crop_Detection_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBu_OjCHml8u"
      },
      "source": [
        "### Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RilxkK7Rml8x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Restart Runtime\n",
        "#import os\n",
        "#os._exit(0)  # This will force restart the runtime\n"
      ],
      "metadata": {
        "id": "WwMwk7u_xnVC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Reinstall the correct versions\n",
        "!pip install --upgrade sympy torchvision torch\n",
        "\n",
        "# Step 3: Import everything again (after installation)\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Step 4: Check if everything works\n",
        "print(\"✅ Torch and torchvision loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "53GSUpgUxpUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d65434-7f7b-4cea-f742-cd64530751fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "✅ Torch and torchvision loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_zip_path = \"/content/Plant_leave_diseases_dataset_without_augmentation.zip\"\n",
        "\n",
        "# Check file type\n",
        "if os.path.exists(dataset_zip_path):\n",
        "    print(\"File exists! Checking type...\")\n",
        "    !file /content/dataset.zip\n",
        "else:\n",
        "    print(\"File does not exist!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odHveMicyKII",
        "outputId": "dbec1d3a-7034-4ee8-c50b-3c1c6d1ec817"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists! Checking type...\n",
            "/content/dataset.zip: cannot open `/content/dataset.zip' (No such file or directory)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t2RuAbmml81"
      },
      "source": [
        "### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BJangQ79ml8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28eaacbc-bfac-4d76-8ef4-960ed76c94b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Extracted files: ['.config', 'Plant_leave_diseases_dataset_without_augmentation.zip', 'Plant_leave_diseases_dataset_without_augmentation', 'sample_data']\n",
            "✅ Dataset loaded successfully!\n",
            "Total Images: 16223\n",
            "Classes: ['Background_without_leaves', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define dataset zip file path\n",
        "dataset_zip_path = \"/content/Plant_leave_diseases_dataset_without_augmentation.zip\"\n",
        "\n",
        "# Extract dataset if not already extracted\n",
        "dataset_root = \"/content/Plant_leave_diseases_dataset_without_augmentation\"  # Check actual folder name\n",
        "\n",
        "if not os.path.exists(dataset_root):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/\")\n",
        "else:\n",
        "    print(\"Dataset already extracted. Skipping extraction.\")\n",
        "\n",
        "# Verify extracted files\n",
        "print(\"Extracted files:\", os.listdir(\"/content/\"))\n",
        "\n",
        "# Define the correct dataset path\n",
        "dataset_path = dataset_root  # Adjust if images are inside a subfolder\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dataset folder not found at {dataset_path}. Check extracted files.\")\n",
        "\n",
        "# Define Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images\n",
        "    transforms.ToTensor()           # Convert to tensor\n",
        "])\n",
        "\n",
        "# Load Dataset\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# Verify Dataset Loaded\n",
        "print(\"✅ Dataset loaded successfully!\")\n",
        "print(f\"Total Images: {len(dataset)}\")\n",
        "print(f\"Classes: {dataset.classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y41PNltml84"
      },
      "source": [
        "### Split into Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bwj4f75Tml82"
      },
      "outputs": [],
      "source": [
        "indices = list(range(len(dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IdMfnEBeml83"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "split = int(np.floor(0.80 * len(dataset)))  # train_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2VRQqUf9ml83"
      },
      "outputs": [],
      "source": [
        "validation = int(np.floor(0.70 * split))  # validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bIM-nvVbml84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923641a3-36b4-40b8-d1b0-ec9fb6008043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 9084 12978 16223\n"
          ]
        }
      ],
      "source": [
        "print(0, validation, split, len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u6acbYavml84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938c0967-3b89-438c-df0d-ff5899a3f02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of train size :9084\n",
            "length of validation size :3894\n",
            "length of test size :7139\n"
          ]
        }
      ],
      "source": [
        "print(f\"length of train size :{validation}\")\n",
        "print(f\"length of validation size :{split - validation}\")\n",
        "print(f\"length of test size :{len(dataset)-validation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XpuSFUPuml84"
      },
      "outputs": [],
      "source": [
        "np.random.shuffle(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_ttvgEcKml84"
      },
      "outputs": [],
      "source": [
        "train_indices, validation_indices, test_indices = (\n",
        "    indices[:validation],\n",
        "    indices[validation:split],\n",
        "    indices[split:],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y0-NXLl0ml85"
      },
      "outputs": [],
      "source": [
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "validation_sampler = SubsetRandomSampler(validation_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "E9ccAIUHml85"
      },
      "outputs": [],
      "source": [
        "targets_size = len(dataset.class_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqBYkHTFml88"
      },
      "source": [
        "### Original Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GgQ7PBA5ml88"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, K):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # conv1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2),\n",
        "            # conv2\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2),\n",
        "            # conv3\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2),\n",
        "            # conv4\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.dense_layers = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(50176, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 9),\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.conv_layers(X)\n",
        "\n",
        "        # Flatten\n",
        "        out = out.view(-1, 50176)\n",
        "\n",
        "        # Fully connected\n",
        "        out = self.dense_layers(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Xx7cWml7ml89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264be3f0-2987-43a8-9aec-eebe741cd2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "f55JW2ssml89"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "F4vHYuzHml89"
      },
      "outputs": [],
      "source": [
        "model = CNN(targets_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DlNsKj3bml8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93a010c-42ad-416a-ff9a-01cc6115efe8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv_layers): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU()\n",
              "    (16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU()\n",
              "    (19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (21): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU()\n",
              "    (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU()\n",
              "    (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (dense_layers): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): Linear(in_features=50176, out_features=1024, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "    (4): Linear(in_features=1024, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fLreCjbiml8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2333a1a-e693-4271-e3d7-75ac411d3d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 224, 224]             896\n",
            "              ReLU-2         [-1, 32, 224, 224]               0\n",
            "       BatchNorm2d-3         [-1, 32, 224, 224]              64\n",
            "            Conv2d-4         [-1, 32, 224, 224]           9,248\n",
            "              ReLU-5         [-1, 32, 224, 224]               0\n",
            "       BatchNorm2d-6         [-1, 32, 224, 224]              64\n",
            "         MaxPool2d-7         [-1, 32, 112, 112]               0\n",
            "            Conv2d-8         [-1, 64, 112, 112]          18,496\n",
            "              ReLU-9         [-1, 64, 112, 112]               0\n",
            "      BatchNorm2d-10         [-1, 64, 112, 112]             128\n",
            "           Conv2d-11         [-1, 64, 112, 112]          36,928\n",
            "             ReLU-12         [-1, 64, 112, 112]               0\n",
            "      BatchNorm2d-13         [-1, 64, 112, 112]             128\n",
            "        MaxPool2d-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15          [-1, 128, 56, 56]          73,856\n",
            "             ReLU-16          [-1, 128, 56, 56]               0\n",
            "      BatchNorm2d-17          [-1, 128, 56, 56]             256\n",
            "           Conv2d-18          [-1, 128, 56, 56]         147,584\n",
            "             ReLU-19          [-1, 128, 56, 56]               0\n",
            "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
            "        MaxPool2d-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 256, 28, 28]         295,168\n",
            "             ReLU-23          [-1, 256, 28, 28]               0\n",
            "      BatchNorm2d-24          [-1, 256, 28, 28]             512\n",
            "           Conv2d-25          [-1, 256, 28, 28]         590,080\n",
            "             ReLU-26          [-1, 256, 28, 28]               0\n",
            "      BatchNorm2d-27          [-1, 256, 28, 28]             512\n",
            "        MaxPool2d-28          [-1, 256, 14, 14]               0\n",
            "          Dropout-29                [-1, 50176]               0\n",
            "           Linear-30                 [-1, 1024]      51,381,248\n",
            "             ReLU-31                 [-1, 1024]               0\n",
            "          Dropout-32                 [-1, 1024]               0\n",
            "           Linear-33                    [-1, 9]           9,225\n",
            "================================================================\n",
            "Total params: 52,564,649\n",
            "Trainable params: 52,564,649\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 143.96\n",
            "Params size (MB): 200.52\n",
            "Estimated Total Size (MB): 345.05\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary\n",
        "\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "B5knXNEgml8-"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()  # this include softmax + cross entropy loss\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Wm9qkhP1ml8_"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kb8da08uml8_"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, sampler=train_sampler,pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, sampler=test_sampler\n",
        ")\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, sampler=validation_sampler\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvQ0LW--ml8_"
      },
      "source": [
        "# Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JdJ98Mqfml8_"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f1(model, data_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)  # Get predicted class\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return f1_score(all_labels, all_preds, average=\"weighted\")  # 'weighted' handles class imbalance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "MXMSr59DVzRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import pickle  # ✅ Added for saving/loading results\n",
        "from google.colab import files  # ✅ Added for auto-download\n",
        "\n",
        "def calculate_accuracy(model, data_loader, device):\n",
        "    \"\"\"Computes accuracy of the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def calculate_f1(model, data_loader, device):\n",
        "    \"\"\"Computes F1-score for model evaluation.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "    return f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "def batch_gd(model, criterion, train_loader, validation_loader, optimizer, device, epochs=20):\n",
        "    \"\"\"Trains a model using batch gradient descent and returns stored training results.\"\"\"\n",
        "    train_losses, validation_losses = [], []\n",
        "    train_accuracies, validation_accuracies = [], []\n",
        "    f1_scores = []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        t0 = datetime.now()\n",
        "        train_loss, train_correct, train_total = [], 0, 0\n",
        "\n",
        "        # Training loop with progress bar\n",
        "        model.train()\n",
        "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {e+1}/{epochs}\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == targets).sum().item()\n",
        "            train_total += targets.size(0)\n",
        "\n",
        "        train_losses.append(np.mean(train_loss))  # Store train loss\n",
        "        train_accuracies.append(train_correct / train_total)  # Store train accuracy\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        validation_loss, val_correct, val_total = [], 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in validation_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                validation_loss.append(loss.item())\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct += (predicted == targets).sum().item()\n",
        "                val_total += targets.size(0)\n",
        "\n",
        "        validation_losses.append(np.mean(validation_loss))  # Store val loss\n",
        "        validation_accuracies.append(val_correct / val_total)  # Store val accuracy\n",
        "        f1_scores.append(calculate_f1(model, validation_loader, device))  # Store F1-score\n",
        "\n",
        "        print(f\"Epoch {e+1}/{epochs} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {validation_losses[-1]:.4f} | \"\n",
        "              f\"Train Acc: {train_accuracies[-1]*100:.2f}% | Val Acc: {validation_accuracies[-1]*100:.2f}% | \"\n",
        "              f\"F1-Score: {f1_scores[-1]:.4f} | Duration: {datetime.now() - t0}\")\n",
        "\n",
        "    return train_losses, validation_losses, train_accuracies, validation_accuracies, f1_scores\n",
        "\n",
        "def plot_metrics(train_losses, val_losses, train_acc, val_acc, f1_scores):\n",
        "    \"\"\"Plots training loss, accuracy, and F1-score and saves it.\"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, train_losses, 'r', label='Train Loss')\n",
        "    plt.plot(epochs, val_losses, 'b', label='Val Loss')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training & Validation Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, train_acc, 'r', label='Train Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Val Accuracy')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training & Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    # F1-Score Plot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, f1_scores, 'g', label='F1 Score')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.title(\"F1-Score over Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_plot.png\")  # ✅ Auto-save the plot\n",
        "    plt.show()\n",
        "\n",
        "    # ✅ Auto-download in Google Colab\n",
        "    files.download(\"training_plot.png\")\n",
        "\n",
        "# ✅ Training Only Once and Saving Results\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train only if results are not already saved\n",
        "try:\n",
        "    with open(\"training_results.pkl\", \"rb\") as f:\n",
        "        train_losses, val_losses, train_acc, val_acc, f1_scores = pickle.load(f)\n",
        "    print(\"✅ Loaded previous training results. No retraining needed.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"🚀 Training model for the first time...\")\n",
        "    train_results = batch_gd(model, criterion, train_loader, validation_loader, optimizer, device, epochs=20)\n",
        "\n",
        "    # Save results to avoid retraining\n",
        "    with open(\"training_results.pkl\", \"wb\") as f:\n",
        "        pickle.dump(train_results, f)\n",
        "\n",
        "    train_losses, val_losses, train_acc, val_acc, f1_scores = train_results\n",
        "\n",
        "# ✅ Plot Metrics Separately (No Retraining)\n",
        "plot_metrics(train_losses, val_losses, train_acc, val_acc, f1_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuo4i07VEIY",
        "outputId": "3c5d47ac-f074-476d-b39d-58637614cf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Training model for the first time...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   4%|▍         | 11/284 [03:41<1:26:01, 18.91s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Values"
      ],
      "metadata": {
        "id": "YTvMu3OPbbS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Losses:\", train_losses)\n",
        "print(\"Validation Losses:\", val_losses)\n",
        "print(\"Train Accuracies:\", train_acc)\n",
        "print(\"Validation Accuracies:\", val_acc)\n",
        "print(\"F1 Scores:\", f1_scores)\n"
      ],
      "metadata": {
        "id": "UQLH6gffZxd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_metrics(train_losses, val_losses, train_acc, val_acc, f1_scores):\n",
        "    # 🔸 Averages\n",
        "    print(\"AVERAGE VALUES ACROSS EPOCHS\")\n",
        "    print(f\"Average Train Loss: {np.mean(train_losses):.4f}\")\n",
        "    print(f\"Average Val Loss:   {np.mean(val_losses):.4f}\")\n",
        "    print(f\"Average Train Acc:  {np.mean(train_acc) * 100:.2f}%\")\n",
        "    print(f\"Average Val Acc:    {np.mean(val_acc) * 100:.2f}%\")\n",
        "    print(f\"Average F1 Score:   {np.mean(f1_scores):.4f}\\n\")\n",
        "\n",
        "    # 🔹 Best (Max accuracy / F1, Min loss)\n",
        "    best_val_acc_idx = np.argmax(val_acc)\n",
        "    best_f1_idx = np.argmax(f1_scores)\n",
        "    lowest_val_loss_idx = np.argmin(val_losses)\n",
        "\n",
        "    print(\"BEST EPOCHS\")\n",
        "    print(f\"Best Val Accuracy:  {val_acc[best_val_acc_idx] * 100:.2f}% (Epoch {best_val_acc_idx + 1})\")\n",
        "    print(f\"Best F1 Score:      {f1_scores[best_f1_idx]:.4f} (Epoch {best_f1_idx + 1})\")\n",
        "    print(f\"Lowest Val Loss:    {val_losses[lowest_val_loss_idx]:.4f} (Epoch {lowest_val_loss_idx + 1})\\n\")\n",
        "\n",
        "    # 🔹 Last Epoch\n",
        "    print(\"LAST EPOCH\")\n",
        "    print(f\"Last Train Loss:    {train_losses[-1]:.4f}\")\n",
        "    print(f\"Last Val Loss:      {val_losses[-1]:.4f}\")\n",
        "    print(f\"Last Train Acc:     {train_acc[-1] * 100:.2f}%\")\n",
        "    print(f\"Last Val Acc:       {val_acc[-1] * 100:.2f}%\")\n",
        "    print(f\"Last F1 Score:      {f1_scores[-1]:.4f}\")\n",
        "\n",
        "# ✅ Call this function after training\n",
        "summarize_metrics(train_losses, val_losses, train_acc, val_acc, f1_scores)\n"
      ],
      "metadata": {
        "id": "t-v7RL-1R9j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Metrics"
      ],
      "metadata": {
        "id": "W38GgK7HWLej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "def summarize_and_export_metrics(\n",
        "    train_losses, val_losses, train_acc, val_acc, f1_scores,\n",
        "    filename=\"detailed_training_summary.csv\"\n",
        "):\n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "    # Build epoch-wise dataframe\n",
        "    df = pd.DataFrame({\n",
        "        \"Epoch\": epochs,\n",
        "        \"Train Loss\": train_losses,\n",
        "        \"Val Loss\": val_losses,\n",
        "        \"Train Acc\": [acc * 100 for acc in train_acc],\n",
        "        \"Val Acc\": [acc * 100 for acc in val_acc],\n",
        "        \"F1 Score\": f1_scores,\n",
        "    })\n",
        "\n",
        "    # Add average row\n",
        "    avg_row = {\n",
        "        \"Epoch\": \"Average\",\n",
        "        \"Train Loss\": np.mean(train_losses),\n",
        "        \"Val Loss\": np.mean(val_losses),\n",
        "        \"Train Acc\": np.mean(train_acc) * 100,\n",
        "        \"Val Acc\": np.mean(val_acc) * 100,\n",
        "        \"F1 Score\": np.mean(f1_scores),\n",
        "    }\n",
        "\n",
        "    # Add best metrics\n",
        "    best_val_acc_idx = np.argmax(val_acc)\n",
        "    best_f1_idx = np.argmax(f1_scores)\n",
        "    lowest_val_loss_idx = np.argmin(val_losses)\n",
        "\n",
        "    best_rows = [\n",
        "        {\n",
        "            \"Epoch\": f\"Best Val Acc (Epoch {best_val_acc_idx+1})\",\n",
        "            \"Train Loss\": train_losses[best_val_acc_idx],\n",
        "            \"Val Loss\": val_losses[best_val_acc_idx],\n",
        "            \"Train Acc\": train_acc[best_val_acc_idx] * 100,\n",
        "            \"Val Acc\": val_acc[best_val_acc_idx] * 100,\n",
        "            \"F1 Score\": f1_scores[best_val_acc_idx],\n",
        "        },\n",
        "        {\n",
        "            \"Epoch\": f\"Best F1 Score (Epoch {best_f1_idx+1})\",\n",
        "            \"Train Loss\": train_losses[best_f1_idx],\n",
        "            \"Val Loss\": val_losses[best_f1_idx],\n",
        "            \"Train Acc\": train_acc[best_f1_idx] * 100,\n",
        "            \"Val Acc\": val_acc[best_f1_idx] * 100,\n",
        "            \"F1 Score\": f1_scores[best_f1_idx],\n",
        "        },\n",
        "        {\n",
        "            \"Epoch\": f\"Lowest Val Loss (Epoch {lowest_val_loss_idx+1})\",\n",
        "            \"Train Loss\": train_losses[lowest_val_loss_idx],\n",
        "            \"Val Loss\": val_losses[lowest_val_loss_idx],\n",
        "            \"Train Acc\": train_acc[lowest_val_loss_idx] * 100,\n",
        "            \"Val Acc\": val_acc[lowest_val_loss_idx] * 100,\n",
        "            \"F1 Score\": f1_scores[lowest_val_loss_idx],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Add last epoch\n",
        "    last_row = {\n",
        "        \"Epoch\": \"Last Epoch\",\n",
        "        \"Train Loss\": train_losses[-1],\n",
        "        \"Val Loss\": val_losses[-1],\n",
        "        \"Train Acc\": train_acc[-1] * 100,\n",
        "        \"Val Acc\": val_acc[-1] * 100,\n",
        "        \"F1 Score\": f1_scores[-1],\n",
        "    }\n",
        "\n",
        "    # Append everything to the final DataFrame\n",
        "    summary_df = pd.concat(\n",
        "        [df, pd.DataFrame([avg_row] + best_rows + [last_row])],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    # Export\n",
        "    summary_df.to_csv(filename, index=False)\n",
        "    print(f\"✅ Full metrics exported to: {filename}\")\n",
        "    files.download(filename)\n"
      ],
      "metadata": {
        "id": "O1dZUiPbcBZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_and_export_metrics(train_losses, val_losses, train_acc, val_acc, f1_scores)\n"
      ],
      "metadata": {
        "id": "VowUqWHAcDM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision\n"
      ],
      "metadata": {
        "id": "86cZj_qlUHKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "def calculate_precision_recall(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    return precision, recall\n",
        "\n",
        "# ✅ Example usage after training\n",
        "precision, recall = calculate_precision_recall(model, validation_loader, device)\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "WxRfEgEdUM-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix\n"
      ],
      "metadata": {
        "id": "kNV7WJcpUUB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def plot_confusion(model, dataloader, class_names, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    # ✅ Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "\n",
        "    # ✅ Plot\n",
        "    fig, ax = plt.subplots(figsize=(20, 20))\n",
        "    disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # ✅ Save and Download\n",
        "    fig.savefig(\"confusion_matrix.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # ✅ Download (Only works in Colab!)\n",
        "    files.download(\"confusion_matrix.png\")\n"
      ],
      "metadata": {
        "id": "YZLtiMSwUXG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Confusion Matrix\n"
      ],
      "metadata": {
        "id": "cwNgROhdX0IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\n",
        "    'Background_without_leaves',\n",
        "    'Tomato___Bacterial_spot',\n",
        "    'Tomato___Early_blight',\n",
        "    'Tomato___Late_blight',\n",
        "    'Tomato___Leaf_Mold',\n",
        "    'Tomato___Septoria_leaf_spot',\n",
        "    'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n",
        "    'Tomato___Tomato_mosaic_virus',\n",
        "    'Tomato___healthy'\n",
        "]\n",
        "\n",
        "plot_confusion(model, validation_loader, class_names, device)\n"
      ],
      "metadata": {
        "id": "aXRPNluAXyo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model\n"
      ],
      "metadata": {
        "id": "-eGTEzjtVFGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict() , 'CDPmodel5.1.pth')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"CDPmodel5.1.pth\")  # Download the model to your local machine\n"
      ],
      "metadata": {
        "id": "f_0T1mzuaFOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM3UJ3LWml9M"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2JdGrR0ml9M"
      },
      "outputs": [],
      "source": [
        "targets_size = 9\n",
        "model = CNN(targets_size)\n",
        "model.load_state_dict(torch.load(\"CDPmodel5.1.pth\"))  # Load weights\n",
        "model.to(device)  # Move to GPU if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_test_data(model, test_loader, device, class_names):\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    print(\"\\n📊 Classification Report on Test Data:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=\"Blues\", xticks_rotation=180)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"test_confusion_matrix.png\")\n",
        "    plt.show()\n",
        "    files.download(\"test_confusion_matrix.png\")\n"
      ],
      "metadata": {
        "id": "PTl2ZJXvfHes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_on_test_data(model, test_loader, device, class_names=[\n",
        "    'Background_without_leaves',\n",
        "    'Tomato___Bacterial_spot',\n",
        "    'Tomato___Early_blight',\n",
        "    'Tomato___Late_blight',\n",
        "    'Tomato___Leaf_Mold',\n",
        "    'Tomato___Septoria_leaf_spot',\n",
        "    'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n",
        "    'Tomato___Tomato_mosaic_virus',\n",
        "    'Tomato___healthy'\n",
        "])\n"
      ],
      "metadata": {
        "id": "vAGf16B6fLp1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}